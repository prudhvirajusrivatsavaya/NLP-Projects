{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8c68be-6239-4ac0-be9c-4b095980f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up your environment:\n",
    "#pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab530a2-c166-4590-b6a1-6b56390f3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, BertTokenizerFast, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62750ac-b384-4df7-8d58-748d168d1133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "num_labels = 17\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "#model = BertForTokenClassification.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=41,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a87b6c-69c9-4ec0-bc34-6f8df3e45ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ner_datasetreference.csv\",encoding='latin-1')\n",
    "df = df.ffill(axis = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acf18bb-ea89-42cb-af26-273862687fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd00daa-6166-4e8f-b10c-2f3430b0c545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sentence: 1', 'Sentence: 2', 'Sentence: 3', ...,\n",
       "       'Sentence: 47957', 'Sentence: 47958', 'Sentence: 47959'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the sentences and the entities in the below format\n",
    "#(\"Apple is looking to buy a startup called XYZ Corp in New York.\", {\"entities\": [(0, 5, \"ORG\"), (35, 42, \"ORG\"), (46, 54, \"GPE\")]}),\n",
    "unique_sent_num =  df['Sentence #'].unique()\n",
    "unique_sent_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587b50da-cb36-443c-8a79-1821354d77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_entities(id):\n",
    "    tempdf = df[df['Sentence #'] == id].reset_index(drop=True)\n",
    "    #entities = [(sent.index(tempdf['Word'][i]),sent.index(tempdf['Word'][i]) + len(sent.index(tempdf['Word'][i])) , tempdf['Tag'][i]) for i in range(len(tempdf)) if tempdf['Tag'][i] != \"O\"]\n",
    "    sent = \" \".join(tempdf['Word']).lower()\n",
    "    sent=sent.replace(\".\", \"\").strip()\n",
    "    #df_dum = tempdf[tempdf['Tag'] !='O'].reset_index(drop=False)\n",
    "    entities_dict =dict()\n",
    "    entities_dict['entities']=[]\n",
    "    sent_backup = sent\n",
    "    sent_length = 0\n",
    "    for j in range(len(tempdf)):\n",
    "        word = tempdf['Word'][j].lower()\n",
    "        pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "        match = re.search(pattern, sent)\n",
    "        if match:\n",
    "            str_ind = match.start()\n",
    "            end_ind = match.end()\n",
    "            entities_dict['entities'].append((str_ind+sent_length, end_ind+sent_length, tempdf['Tag'][j]))\n",
    "            sent, sent_length = sent[end_ind:], len(sent[:end_ind])+sent_length\n",
    "    return (sent_backup, entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eff060e-8c73-42f8-9300-50b43c50c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create=False\n",
    "if create:\n",
    "    input = [] \n",
    "    for i in tqdm(unique_sent_num):\n",
    "        input.append(sentence_entities(i))\n",
    "    #input = list(map(sentence_entities,unique_sent_num))\n",
    "    # Open a file and use dump()\n",
    "    with open('ner_input.pkl', 'wb') as file:\n",
    "        # A new file will be created\n",
    "        pickle.dump(input, file)  \n",
    "else:\n",
    "    file = open(\"ner_input_sep14.pkl\",'rb')\n",
    "    input = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492bb5cc-6379-4f26-8a36-370588b564ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country',\n",
       " {'entities': [(0, 9, 'O'),\n",
       "   (10, 12, 'O'),\n",
       "   (13, 26, 'O'),\n",
       "   (27, 31, 'O'),\n",
       "   (32, 39, 'O'),\n",
       "   (40, 47, 'O'),\n",
       "   (48, 54, 'B-geo'),\n",
       "   (55, 57, 'O'),\n",
       "   (58, 65, 'O'),\n",
       "   (66, 69, 'O'),\n",
       "   (70, 73, 'O'),\n",
       "   (74, 76, 'O'),\n",
       "   (77, 81, 'B-geo'),\n",
       "   (82, 85, 'O'),\n",
       "   (86, 92, 'O'),\n",
       "   (93, 96, 'O'),\n",
       "   (97, 107, 'O'),\n",
       "   (108, 110, 'O'),\n",
       "   (111, 118, 'B-gpe'),\n",
       "   (119, 125, 'O'),\n",
       "   (126, 130, 'O'),\n",
       "   (131, 135, 'O'),\n",
       "   (136, 143, 'O')]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = input[0:100]\n",
    "input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49afdfde-fbff-46b3-b422-5bbb4c490e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your labels and map them to numerical IDs\n",
    "labels = ['O','B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0489fc3-f5c2-41ad-93bd-6b3f29c3a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "wrong =[]\n",
    "for i in tqdm(range(len(input))):\n",
    "    text =  input[i][0]\n",
    "    entities = []\n",
    "    for j in input[i][1]['entities']:\n",
    "        entities.append(j[2])\n",
    "    sap_data = dict()\n",
    "    text = text.split()\n",
    "    text.insert(len(text),\"[SEP]\")\n",
    "    text.insert(0,\"[CLS]\")\n",
    "    sap_data['tokens'] = text\n",
    "    entities.insert(len(entities),\"O\")\n",
    "    entities.insert(0,\"O\")\n",
    "    sap_data['labels'] = entities\n",
    "    if len(entities)!=len(text):\n",
    "        #print(i)\n",
    "        wrong.append(i)\n",
    "    train_data.append(sap_data)\n",
    "len(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ce8980-b3e8-465e-938e-2dbcd5ef2951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['[CLS]',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'demonstrators',\n",
       "  'have',\n",
       "  'marched',\n",
       "  'through',\n",
       "  'london',\n",
       "  'to',\n",
       "  'protest',\n",
       "  'the',\n",
       "  'war',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  'and',\n",
       "  'demand',\n",
       "  'the',\n",
       "  'withdrawal',\n",
       "  'of',\n",
       "  'british',\n",
       "  'troops',\n",
       "  'from',\n",
       "  'that',\n",
       "  'country',\n",
       "  '[SEP]'],\n",
       " 'labels': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-geo',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-geo',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-gpe',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a11e6b6-bb10-43fe-b3f7-9cd72919db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode your training data\n",
    "# train_data = [\n",
    "#     {\"tokens\": [\"[CLS]\", \"John\", \"lives\", \"in\", \"New\", \"York\", \"[SEP]\"],\n",
    "#      \"labels\": [\"O\", \"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\"]}\n",
    "#     # Add more examples\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bcd18a5-9550-4a23-8c44-9b29cf8ff4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    inputs = tokenizer(data[\"tokens\"], is_split_into_words=True, return_offsets_mapping=True, padding=\"max_length\", truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    labels = [label2id[label] for label in data[\"labels\"]]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8785d29f-b402-4fee-a2ed-b83f0e275bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encode the training data\n",
    "train_data = [encode_data(example) for example in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f164c665-18b7-4c6e-8724-f3d89f465c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\nlp_2\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3aa1cf-8fa3-479c-bc3b-b2bf7f420acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36a5c719-0239-40d7-98bc-16393d56a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f4c6d22-2cdb-4f3f-8a07-0ea372e809f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels supported by the model: 41\n"
     ]
    }
   ],
   "source": [
    "# Access the model's configuration\n",
    "config = model.config\n",
    "# Check the number of labels supported by the model\n",
    "num_labels = config.num_labels\n",
    "print(f\"Number of labels supported by the model: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3111bc2-9ad9-4500-8ab8-06adecae68bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1c0a349-8169-42ae-8048-8e7e7d8866a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7967f-af4f-460d-b786-f2c109902221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Create a trainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset = train_data\n",
    ")\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f1093-c917-433a-976b-fda4c808c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4cf73ea-5a27-4781-a646-9110856666df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a directory\n",
    "model.save_pretrained(\"NER_TRANSFORMER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4b87108-87a2-4904-bd4d-064a41062828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from the saved directory\n",
    "loaded_model = BertForTokenClassification.from_pretrained(\"NER_TRANSFORMER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64eaa47a-3b33-48d9-947a-6e1f11bba184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for prediction\n",
    "def predict_ner(sentence, model, tokenizer):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "    predicted_labels = [model.config.id2label[label_id] for label_id in predicted_labels]\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17c502e5-c29c-418a-b145-0bc8f6dc6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_ner(sentence, model, tokenizer):\n",
    "    # Tokenize and encode the input sentence\n",
    "    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "    predicted_labels = [model.config.id2label[label_id] for label_id in predicted_labels]\n",
    "\n",
    "    # Get the start and end indices of the entities\n",
    "    entity_start = []\n",
    "    entity_end = []\n",
    "    current_entity_start = None\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        if label != \"O\":  # \"O\" indicates no entity\n",
    "            if label.startswith(\"B-\"):  # Beginning of an entity\n",
    "                entity_start.append(i)\n",
    "                current_entity_start = i\n",
    "            elif label.startswith(\"I-\") and current_entity_start is not None:\n",
    "                entity_end.append(i)\n",
    "            else:\n",
    "                entity_start.append(i)  # Handle single-token entities\n",
    "                entity_end.append(i)\n",
    "                current_entity_start = None\n",
    "\n",
    "    entities = []\n",
    "    for start, end, label in zip(entity_start, entity_end, predicted_labels):\n",
    "        entity_text = tokenizer.decode(inputs[\"input_ids\"][0][start:end+1])\n",
    "        # if entity_text in ['[CLS]','[SEP]']:\n",
    "        #     add = 0\n",
    "        #     start =0\n",
    "        # else:\n",
    "        #     start = start-1\n",
    "        #     add = len(entity_text)-1\n",
    "        entities.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"label\": label,\n",
    "            \"text\": entity_text\n",
    "        })\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09c876ba-7715-4ac0-8576-b5959dcccbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0', 'LABEL_0']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence for prediction\n",
    "new_sentence = \"John lives in Google New York and he works at Google\"\n",
    "\n",
    "# Make predictions\n",
    "predicted_labels = predict_ner(new_sentence, loaded_model, tokenizer)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c969cc4f-607a-4298-b422-8e04be7d1dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels = [id2label[int(label.split(\"_\")[1])] for label in predicted_labels]\n",
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c744b58-198a-4a0a-b07b-361d07039d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
