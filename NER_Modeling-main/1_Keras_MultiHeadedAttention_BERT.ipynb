{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3ce2572d-0c8a-466b-86c1-1dee1950ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/ner_transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "321263df-2552-4835-a887-eca09ea34e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "efd1c880-5121-431d-9aaf-f4457d1e1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_data = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a4a80d88-1975-436d-8305-e43209a24328",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      7\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m      8\u001b[0m                     \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m      9\u001b[0m                     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m                     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m                 )\n\u001b[1;32m---> 17\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m export_to_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/conll_train.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, conll_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     19\u001b[0m export_to_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/conll_val.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, conll_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'data'"
     ]
    }
   ],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            if len(tokens) > 0:\n",
    "                f.write(\n",
    "                    str(len(tokens))\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(tokens)\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(map(str, ner_tags))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "os.mkdir(\"data\")\n",
    "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9e010567-5b38-4bbd-8ae9-f5001780f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0a518035-5e0e-46d9-a6e9-74337f3047de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# The StringLook class will convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "74786b74-7e02-4dc0-985e-a7e64ed6857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0d6b8d2e-30ec-4264-8909-89e8bf0df3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "eb828873-8c2e-4f78-8d0a-c3ba32e305f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "    \n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "# We use `padded_batch` here because each record in the dataset has a\n",
    "# different length.\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cde168d7-64ef-4ec7-8186-17be6184a924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 16), dtype=int64, numpy=\n",
       " array([[  512,  8781,  8782,    10,     2,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  2447,    30,  2717,  8245,     9,  1098,  4729,   391,\n",
       "             3,   512,  3046,   714,     3,  1486,  6347],\n",
       "        [ 1042,    38,    67,    10,   194,  2229,     2,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   87,   320,    17,  1077,     2,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  2230,    30,  2448,  5253,     9,  1324,  5914,  3452,\n",
       "             3,  1324,  4730,   352,   452,     0,     0],\n",
       "        [  899, 13808, 13809,     3,  1400, 13810, 13811,    10,     2,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1769, 13812,     2,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  1904,    30,  2718,  4237,     8,  3126,     9,  1486,\n",
       "         13813, 13814,    10,     2,     0,     0,     0],\n",
       "        [ 2719,  8783,     9,  3173,  6348,  5255,     3,  1098, 13815,\n",
       "           345,    38,    67,    10,     8,     0,     0],\n",
       "        [13816,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  3454,  2024,    30,  2449,  8784,   720,     9,  1041,\n",
       "          5915,     0,     0,     0,     0,     0,     0],\n",
       "        [ 4731,     3,   512,  5916,  4013,     3,  2450,  3559,  1255,\n",
       "           452,  5256,  4781, 13817,    10,     2,     0],\n",
       "        [ 1770,  8785,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [    9,   760,  1665,   907,    38,    67,    10,     2,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  4732,    30,  1771,  3174,     9,   760,  3455,  3175,\n",
       "             3,  6349,  8247,   731,    38,     0,     0],\n",
       "        [   67,     3,  2450,   928,  1042,   452,  1257,  8248,  8249,\n",
       "             3,   899,  8250,  8251,    10,     2,     0],\n",
       "        [ 2451,  8786,     9,  3173,  8787,  1042,    10,     2,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   18,  3456,    30,  1772,  6350,   720,     9,  1486,   460,\n",
       "          4238,     3,     0,     0,     0,     0,     0],\n",
       "        [ 1257,  5011,  3176,    38,    67,     3,   512,  4014,   345,\n",
       "             3,   512,  4733,  1255,    10,     2,     0],\n",
       "        [ 1773,  5254,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [    9,  1195,  3656,  2812,     3,   512,  8788,   391,   452,\n",
       "          1098,  2143,  8789,    10,     8, 13818,     0],\n",
       "        [    9,  1195,  1620,   550,    38,    67,     3,  1195,  3656,\n",
       "           936,    10,     2,     0,     0,     0,     0],\n",
       "        [   85,    17,  1489,    96,    77,     2,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 4016,   347,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [   77,     4,  1489,    96,    85,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  225,    12,   143,    30,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  713,    88,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 8790,    21,  8791,    21,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 8792,    16,  4737,    22,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 3768,    21,  6664,    22,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [ 1327,    16,  3260,   168,    21,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [  721,   111,    30,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(32, 16), dtype=int64, numpy=\n",
       " array([[2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 1, 4, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3],\n",
       "        [1, 1, 1, 1, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 1, 4, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 0, 0],\n",
       "        [2, 3, 1, 1, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 1, 4, 1, 1, 1, 1, 2, 3, 1, 1, 1, 0, 0, 0],\n",
       "        [4, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 7, 1, 4, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 0],\n",
       "        [4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 1, 4, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 0, 0],\n",
       "        [1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 0],\n",
       "        [4, 1, 1, 2, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 6, 1, 4, 1, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [2, 3, 1, 1, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 0],\n",
       "        [4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 2, 3, 1, 1, 2, 3, 1, 1, 2, 3, 1, 1, 1, 1, 0],\n",
       "        [1, 2, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 8, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 8, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 1, 4, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_iterator = iter(train_dataset)\n",
    "first_batch = next(train_iterator)\n",
    "tokens, tags = first_batch\n",
    "tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5d142-7219-4361-911a-3955bbe06ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0ebf1b94-ae40-49d6-8a37-f669129ea046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4ac96ca3-db53-4c1e-960d-dc6f502ef4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(num_heads=2, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2c88a3d5-f7c9-445e-a42e-a493121d0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c7e16f02-e0da-401d-ab02-5b9a0d3c9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "18fa1901-2d90-46c5-a264-88804f4cf84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b14fd7c7-351d-4322-ab83-c4ebdbd39ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 10s 19ms/step - loss: 0.6225\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 9s 21ms/step - loss: 0.2509\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.1542\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.1214\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.0955\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 10s 24ms/step - loss: 0.0782\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.0612\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.0532\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.0472\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.0424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23df49e10d0>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n",
    "padding_token = 0\n",
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "73f4ea96-08e2-4205-8b1d-95d431325316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 215 5773]], shape=(1, 2), dtype=int64)\n",
      "1/1 [==============================] - 1s 877ms/step\n",
      "['B-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n",
    "    \n",
    "# Sample inference using the trained model\n",
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"british lamb\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)\n",
    "\n",
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "64381901-c971-4c2c-a9e4-b0b4a97ecd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>215</td>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1\n",
       "0  215  5773"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "73409004-3d59-4b64-9e2b-f5b4c43db3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.1546319e-09, 1.5470059e-05, 8.0001610e-06, 1.0159526e-06,\n",
       "         9.6417809e-01, 4.6157079e-06, 1.0047689e-03, 8.0988531e-09,\n",
       "         3.4787692e-02, 2.8634406e-07],\n",
       "        [8.4100918e-07, 7.2046286e-01, 1.2903955e-06, 3.5517319e-04,\n",
       "         1.8976832e-06, 2.7638689e-01, 7.9034222e-04, 1.9615162e-03,\n",
       "         3.0298065e-06, 3.6155077e-05]]], dtype=float32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "488f192b-70b0-4e05-8ef3-20b7aa954483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 429ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "    \n",
    "    #evaluate(real_tags, predicted_tags)\n",
    "    return real_tags, predicted_tags\n",
    "\n",
    "real_tags, predicted_tags = calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ada3ac69-2248-453f-a534-98e4fd90344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(real_tags[0], predicted_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064c278-2603-49e5-8e6d-c73549035af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
