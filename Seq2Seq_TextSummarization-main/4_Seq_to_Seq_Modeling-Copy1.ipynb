{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74e52ac-47e7-4fc3-8cb8-41b72f1e24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca33e5f8-946a-484f-ac7e-4e7c721983b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\User\\\\Downloads\\\\fra-eng\\\\fra.txt', 'r',encoding=\"utf-8\") as file:\n",
    "    # Read the entire file content into a string\n",
    "    lines = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0632b918-5b38-47f9-971b-03cab2154dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store the French and English sentences\n",
    "french_sentences = []\n",
    "english_sentences = []\n",
    "for line in lines:\n",
    "    if '\\t' in line:\n",
    "        french, english, _ = line.split('\\t')\n",
    "        french_sentences.append(french)\n",
    "        english_sentences.append(english)\n",
    "\n",
    "french_sentences = french_sentences[0:1000]\n",
    "english_sentences = english_sentences[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ea7d12-e1f4-4946-b4bc-18e8b860589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text (convert words to integer IDs)\n",
    "french_tokenizer = Tokenizer()\n",
    "french_tokenizer.fit_on_texts(french_sentences)\n",
    "french_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
    "\n",
    "english_tokenizer = Tokenizer()\n",
    "english_tokenizer.fit_on_texts(english_sentences)\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
    "\n",
    "# Add special start and end tokens to the English tokenizer\n",
    "start_token = '<start>'\n",
    "end_token = '<end>'\n",
    "english_tokenizer.word_index[start_token] = len(english_tokenizer.word_index) + 1\n",
    "english_tokenizer.index_word[len(english_tokenizer.word_index)] = start_token\n",
    "english_tokenizer.word_index[end_token] = len(english_tokenizer.word_index) + 1\n",
    "english_tokenizer.index_word[len(english_tokenizer.word_index)] = end_token\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = max(len(seq) for seq in french_sequences + english_sequences)\n",
    "french_sequences = pad_sequences(french_sequences, maxlen=max_sequence_length, padding='post')\n",
    "english_sequences = pad_sequences(english_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Create one-hot encoding for English sequences\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "english_sequences_one_hot = tf.keras.utils.to_categorical(english_sequences, num_classes=english_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd1b8bbc-5246-494a-94e7-348f741f9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_french, X_val_french, X_train_english, X_val_english, y_train, y_val = train_test_split(\n",
    "    french_sequences, english_sequences, english_sequences_one_hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f19769c5-6b41-4479-8297-e0456fec3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Seq2Seq model\n",
    "latent_dim = 256  # Adjust as needed\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_sequence_length,))\n",
    "encoder_embedding = Embedding(input_dim=len(french_tokenizer.word_index) + 1, output_dim=latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_sequence_length,))\n",
    "decoder_embedding = Embedding(input_dim=len(english_tokenizer.word_index) + 1, output_dim=latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(english_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60f91d76-9d96-4bca-aad3-5a8efd029710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 5s 121ms/step - loss: 5.2151 - accuracy: 0.5902 - val_loss: 2.6271 - val_accuracy: 0.6375\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 1s 58ms/step - loss: 2.6143 - accuracy: 0.6377 - val_loss: 2.6118 - val_accuracy: 0.6375\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 2.3599 - accuracy: 0.6377 - val_loss: 2.5117 - val_accuracy: 0.6375\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 1s 58ms/step - loss: 2.2570 - accuracy: 0.6458 - val_loss: 2.4634 - val_accuracy: 0.6567\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 2.1779 - accuracy: 0.6556 - val_loss: 2.4300 - val_accuracy: 0.6567\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 2.1013 - accuracy: 0.6571 - val_loss: 2.3823 - val_accuracy: 0.6567\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 1s 60ms/step - loss: 2.0352 - accuracy: 0.6581 - val_loss: 2.3354 - val_accuracy: 0.6683\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 1s 59ms/step - loss: 1.9772 - accuracy: 0.6685 - val_loss: 2.3002 - val_accuracy: 0.6692\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 1s 64ms/step - loss: 1.9215 - accuracy: 0.6704 - val_loss: 2.2598 - val_accuracy: 0.6692\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 1s 61ms/step - loss: 1.8660 - accuracy: 0.6700 - val_loss: 2.2201 - val_accuracy: 0.6692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16bdaac7be0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit([X_train_french, X_train_english], y_train,\n",
    "          validation_data=([X_val_french, X_val_english], y_val),\n",
    "          batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "121090b0-169f-44dd-b636-bcd0d6d49e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model for inference/translation\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9027f1f-ae4d-44a8-a908-d7b1b81f09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3489d86-c2f5-4589-981f-7fd3851a2ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdecoder_inputs\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "662e5875-2cad-42e4-8185-eaf6a2c717a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Save the encoder model\n",
    "encoder_model.save(\"encoder_model.h5\")\n",
    "\n",
    "# Save the decoder model\n",
    "decoder_model.save(\"decoder_model.h5\")\n",
    "\n",
    "# Save the tokenizers\n",
    "with open(\"french_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(french_tokenizer, f)\n",
    "\n",
    "with open(\"english_tokenizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(english_tokenizer, f)\n",
    "\n",
    "# Save vocabulary and special tokens information\n",
    "vocab_info = {\n",
    "    \"french_tokenizer_word_index\": french_tokenizer.word_index,\n",
    "    \"english_tokenizer_word_index\": english_tokenizer.word_index,\n",
    "    \"start_token\": start_token,\n",
    "    \"end_token\": end_token,\n",
    "}\n",
    "\n",
    "with open(\"vocab_info.pickle\", \"wb\") as f:\n",
    "    pickle.dump(vocab_info, f)\n",
    "\n",
    "# Save maximum sequence length\n",
    "with open(\"max_sequence_length.txt\", \"w\") as f:\n",
    "    f.write(str(max_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7f6028-b65b-41ab-89c3-5bfb341754b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "encoder_model = load_model(\"encoder_model.h5\")\n",
    "\n",
    "# Load the trained model\n",
    "decoder_model = load_model(\"decoder_model.h5\")\n",
    "\n",
    "# Load the saved tokenizers\n",
    "with open(\"french_tokenizer.pickle\", \"rb\") as f:\n",
    "    french_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"english_tokenizer.pickle\", \"rb\") as f:\n",
    "    english_tokenizer = pickle.load(f)\n",
    "\n",
    "# Load vocabulary and special tokens information\n",
    "with open(\"vocab_info.pickle\", \"rb\") as f:\n",
    "    vocab_info = pickle.load(f)\n",
    "\n",
    "start_token = vocab_info[\"start_token\"]\n",
    "end_token = vocab_info[\"end_token\"]\n",
    "\n",
    "# Load maximum sequence length\n",
    "with open(\"max_sequence_length.txt\", \"r\") as f:\n",
    "    max_sequence_length = int(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f95d42-3bef-460a-ae62-90322c46cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text):\n",
    "    input_seq = french_tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_sequence_length, padding='post')\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = english_tokenizer.word_index[start_token]  # Start token\n",
    "    \n",
    "    stop_condition = False\n",
    "    translation = []\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Handle OOV tokens\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_word = end_token  # Replace with your choice of fallback token\n",
    "        else:\n",
    "            sampled_word = english_tokenizer.index_word.get(sampled_token_index, end_token)\n",
    "        \n",
    "        translation.append(sampled_word)\n",
    "        \n",
    "        if sampled_word == end_token or len(translation) > max_sequence_length:\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932261ba-6dd7-4dec-8ca2-4b93508b2ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 383ms/step\n",
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "French Input: Go.\n",
      "English Translation: je je <end>\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_text = \"Go.\"  # Replace with your own French text\n",
    "translation = translate(input_text)\n",
    "print(\"French Input:\", input_text)\n",
    "print(\"English Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000a33c-cf1f-4933-8421-fd552bac0028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
